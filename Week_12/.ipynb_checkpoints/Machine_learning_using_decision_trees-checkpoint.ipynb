{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Trees and Machine Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Decision trees are tree structures containing rules\n",
    "<li>The leaf nodes of the tree are the \"learned\" categories (or threshold values)\n",
    "<li>A path from the root to a leaf node represents a rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Example: A decision tree with rules on deciding who survived or died on the titanic</h3>\n",
    "<i>Source: https://en.wikipedia.org/wiki/Decision_tree_learning#/media/File:CART_tree_titanic_survivors.png</i>\n",
    "<li>To use the tree, enter a person-data object and you'll get an answer</li>\n",
    "<li>Ex: (\"John Brown\",\"Male\",\"30 years old\", \"3 siblings\") Ans: Survived (89% probability)\n",
    "<li>Ex: (\"Jill Jones\", \"Female\",\"7 years old\", \"no siblings\") Ans: Survived (73% probability)\n",
    "<li>Ex: (\"Hercules Mulligan\", \"Male\", \"2 years old\", \"20 siblings\") Ans: Died (17% probability)\n",
    "<li>Note that the 17% probability doesn't mean that there is an 83% chance that Mulligan survived!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"CART_tree_titanic_survivors.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Types of decision trees</h2>\n",
    "<ul>\n",
    "<li><b>Classification trees</b>: Uses rules to classify cases into two or more categories (Rocks vs Mines)\n",
    "<ul>\n",
    "<li>Classification trees recursively split the data on a feature value\n",
    "<li>Each split minimizes the cost (also known as the impurity)\n",
    "<li>Cost is commonly measured using the GINI cost function (a measure of the probability of misclassification or 'purity')\n",
    "</ul>\n",
    "<li><b>Regression trees</b>: Uses rules to group data into target variable ranges (Wine Quality)\n",
    "<ul>\n",
    "<li>Also split the data on feature values\n",
    "<li>Minimize cost (impurity). Usually the mean squared error\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stopping and Pruning Rules</h3>\n",
    "<li>A minimum count of observations in each leaf node\n",
    "<li>A maximum tree <b>depth</b>\n",
    "<li>A maximum <b>complexity</b> (the number of splits)\n",
    "<li>Using all three, you won't necessarily have a balanced tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predicting wine quality using a decision tree</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "w_df = pd.read_csv(url,header=0,sep=';')\n",
    "w_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Build train and test samples</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      quality\n",
       "1197        6\n",
       "611         5\n",
       "898         7\n",
       "872         4\n",
       "1217        6\n",
       "1049        6\n",
       "1518        5\n",
       "1041        6\n",
       "1513        6\n",
       "150         6\n",
       "1416        5\n",
       "1563        5\n",
       "1151        6\n",
       "605         6\n",
       "1021        6\n",
       "984         5\n",
       "1428        5\n",
       "744         5\n",
       "1531        5\n",
       "322         5\n",
       "1363        4\n",
       "437         6\n",
       "1362        6\n",
       "221         5\n",
       "919         6\n",
       "127         5\n",
       "269         6\n",
       "1366        5\n",
       "1075        7\n",
       "564         6\n",
       "...       ...\n",
       "665         5\n",
       "104         5\n",
       "463         5\n",
       "1454        6\n",
       "1588        6\n",
       "1424        6\n",
       "860         5\n",
       "1545        6\n",
       "632         6\n",
       "682         5\n",
       "1535        6\n",
       "604         6\n",
       "1256        5\n",
       "230         7\n",
       "1397        5\n",
       "902         7\n",
       "856         6\n",
       "184         6\n",
       "461         5\n",
       "712         5\n",
       "313         5\n",
       "589         7\n",
       "915         6\n",
       "568         6\n",
       "770         6\n",
       "141         5\n",
       "1388        5\n",
       "1209        7\n",
       "1281        6\n",
       "582         5\n",
       "\n",
       "[480 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(w_df, test_size = 0.3)\n",
    "x_train = train.iloc[0:,0:11]\n",
    "y_train = train[['quality']]\n",
    "x_test = test.iloc[0:,0:11]\n",
    "y_test = test[['quality']]\n",
    "\n",
    "#Use all data for cross validation\n",
    "x_data = w_df.iloc[0:,0:11]\n",
    "y_data = w_df[['quality']]\n",
    "#x_data\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classifiers vs Regressors</h3>\n",
    "<li>Decision tree regressors are used when the target variable is continuous and ordered (wine quality from 0 to 10)\n",
    "<li>Classifiers are used when the target variable is a set of unordered categories (rocks or mines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For wine quality, we need a regressor</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth = 3)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R-Square 0.366116252101\n",
      "Testing R-Square 0.235114230276\n"
     ]
    }
   ],
   "source": [
    "#Get the R-Square for the predicted vs actuals on the text sample\n",
    "print(\"Training R-Square\",model.score(x_train,y_train))\n",
    "print(\"Testing R-Square\",model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>View the tree</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Download and install <a href=\"http://www.graphviz.org/Download.php\">graphviz</a></h3>\n",
    "If you are having issues using Graphviz in Windows, then try the following steps:\n",
    "<ol>\n",
    "<li>1. Install Graphviz \n",
    "<li>2. After installing graphviz, add it to the Computer's Path. \n",
    "<ul>\n",
    "<li>Go to PC properties \n",
    "<li> Click environment variables in the advanced settings section\n",
    "<li> Add C:\\Program Files (x86)\\Graphviz2.38\\bin\\ to the PATH and click Apply\n",
    "</ul>\n",
    "<li> Install Pydotplus. Note that you will always have to install pydot after graphviz as Pydot is Graphviz's dot language and needs Graphviz for reference. \n",
    "</ol>\n",
    "<h3>Install pydotplus (using pip): Install graphviz before you install pydotplus!</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "feature_names = [key for key in w_df]\n",
    "dot_data = tree.export_graphviz(model.tree_, out_file=None,feature_names=feature_names) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "graph.write_pdf(\"wines.pdf\") \n",
    "#The tree will be saved to wines.pdf in your current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision trees are Entropy minimizers</h3>\n",
    "<li><b>Entropy</b>: a measure of uncertainty in the data<p>\n",
    "what is the uncertainty in color when you draw a marble from a box of 100 blue marbles?<p>\n",
    "what is the uncertainty when you draw a marble from a box with 50 blue and 50 red marbles?\n",
    "<li>Entropy minimization: decision tree algorithms seek to partition the data on features in the way that total entropy is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Regression trees</h3>\n",
    "<li>Run regressions for each X to the dependent variable\n",
    "<li>Pick the variable with the most explanatory power and split it at several points\n",
    "<li>Calculate the Mean Square Error of each of the two halves for each split\n",
    "<li>Pick the split point that gives the lowest mse (combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cross validation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Split the training set into k smaller sets (aka folds)\n",
    "<li>Train the data on k-1 folds\n",
    "<li>Validate the results on fold k\n",
    "<li>Repeat this holding out each of the k folds in turn\n",
    "<li>Report the average of all tests as the performance metric\n",
    "<li>http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.cross_validation import cross_val_score\n",
    "#from sklearn.cross_validation import KFold\n",
    "crossvalidation = KFold(n_splits=5,shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "for depth in range(1,10):\n",
    "    model = tree.DecisionTreeRegressor(\n",
    "    max_depth=depth, random_state=0)\n",
    "    if model.fit(x_data,y_data).tree_.max_depth < depth:\n",
    "        break\n",
    "    score = np.mean(cross_val_score(model, x_data, y_data,scoring='neg_mean_squared_error', cv=crossvalidation, n_jobs=1))\n",
    "    print ('Depth: %i Accuracy: %.3f' % (depth,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Purpose of cross-validation</h3>\n",
    "<li>Not to generate a tree (it generates many trees!)\n",
    "<li>But to provide an estimate of the average error of the model\n",
    "<li>Roughly, the idea is to see how the model performance varies with different training sets\n",
    "<li>To generate the tree, use the entire training set as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification trees</h1>\n",
    "<h2>Classification trees are used when dealing with categorical dependent variables</h2>\n",
    "<li>Pick a variable and a split point so that the misclassification cost is the lowest.\n",
    "\n",
    "<h3>Rocks and mines data set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "df = pd.read_csv(url,header=None)\n",
    "df[60]=np.where(df[60]=='R',0,1)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.3)\n",
    "x_train = train.iloc[0:,0:60]\n",
    "y_train = train[[60]]\n",
    "x_test = test.iloc[0:,0:60]\n",
    "y_test = test[[60]]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth = 3,criterion='entropy')\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "feature_names = [key for key in df]\n",
    "dot_data = tree.export_graphviz(model.tree_, out_file=None,feature_names=feature_names) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "graph.write_pdf(\"mines.pdf\") \n",
    "#The tree will be saved to mines.pdf in your current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predicted, actual, threshold):\n",
    "    if len(predicted) != len(actual): return -1\n",
    "    tp = 0.0\n",
    "    fp = 0.0\n",
    "    tn = 0.0\n",
    "    fn = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] > 0.5: #labels that are 1.0  (positive examples)\n",
    "            if predicted[i] > threshold:\n",
    "                tp += 1.0 #correctly predicted positive\n",
    "            else:\n",
    "                fn += 1.0 #incorrectly predicted negative\n",
    "        else:              #labels that are 0.0 (negative examples)\n",
    "            if predicted[i] < threshold:\n",
    "                tn += 1.0 #correctly predicted negative\n",
    "            else:\n",
    "                fp += 1.0 #incorrectly predicted positive\n",
    "    rtn = [tp, fn, fp, tn]\n",
    "\n",
    "    return rtn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train=model.predict(x_train)\n",
    "p_test = model.predict(x_test)\n",
    "print(confusion_matrix(p_train,np.array(y_train),.5))\n",
    "print(confusion_matrix(p_test,np.array(y_test),.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "(fpr, tpr, thresholds) = roc_curve(y_train,p_train)\n",
    "area = auc(fpr,tpr)\n",
    "pl.clf() #Clear the current figure\n",
    "pl.plot(fpr,tpr,label=\"In-Sample ROC Curve with area = %1.2f\"%area)\n",
    "\n",
    "pl.plot([0, 1], [0, 1], 'k') #This plots the random (equal probability line)\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('In sample ROC rocks versus mines')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "import pylab as pl\n",
    "%matplotlib inline\n",
    "(fpr, tpr, thresholds) = roc_curve(y_test,p_test)\n",
    "area = auc(fpr,tpr)\n",
    "pl.clf() #Clear the current figure\n",
    "pl.plot(fpr,tpr,label=\"Out-Sample ROC Curve with area = %1.2f\"%area)\n",
    "\n",
    "pl.plot([0, 1], [0, 1], 'k') #This plots the random (equal probability line)\n",
    "pl.xlim([0.0, 1.0])\n",
    "pl.ylim([0.0, 1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('Out sample ROC rocks versus mines')\n",
    "pl.legend(loc=\"lower right\")\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
